{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.datasets.ebhi_seg_dataset import EBHISegDataset\n",
        "from src.datasets.transforms import get_train_transforms\n",
        "from src.models.unet import UNet\n",
        "from src.models.losses import combined_loss\n",
        "from src.utils.paths import load_config, get_data_paths, resolve_path\n",
        "from src.utils.seed import set_seed\n",
        "\n",
        "cfg = load_config('config/default.yaml')\n",
        "paths = get_data_paths(cfg)\n",
        "\n",
        "train_split = resolve_path(paths['splits'] / 'train.txt')\n",
        "ids = [l.strip() for l in train_split.read_text().splitlines() if l.strip()]\n",
        "small_ids = ids[:8]\n",
        "(tmp := (paths['splits'] / 'small_train.txt')).write_text('\\n'.join(small_ids))\n",
        "\n",
        "image_size = int(cfg['data']['image_size'])\n",
        "mean = cfg['data']['normalization']['mean']\n",
        "std = cfg['data']['normalization']['std']\n",
        "transform = get_train_transforms(image_size, mean, std)\n",
        "\n",
        "ignore_index = int(cfg['loss'].get('ignore_index', -1))\n",
        "set_seed(int(cfg['training'].get('seed', 42)))\n",
        "\n",
        "dataset = EBHISegDataset(\n",
        "    split_file=str(tmp),\n",
        "    images_dir=str(paths['processed_images']),\n",
        "    masks_dir=str(paths['processed_masks']),\n",
        "    transform=transform,\n",
        "    ignore_index=ignore_index,\n",
        ")\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet(in_channels=3, num_classes=cfg['data']['num_classes'], base_channels=16).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for batch in loader:\n",
        "        imgs = batch['image'].to(device)\n",
        "        masks = batch['mask'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = combined_loss(logits, masks, dice_weight=cfg['loss'].get('dice_weight', 1.0), ignore_index=ignore_index)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item() * imgs.size(0)\n",
        "    epoch_loss = running / len(dataset)\n",
        "    loss_history.append(epoch_loss)\n",
        "    if epoch % 5 == 0:\n",
        "        print(f'Epoch {epoch}: loss={epoch_loss:.4f}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Tiny subset overfit sanity check')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
